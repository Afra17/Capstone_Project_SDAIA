{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b047f9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install crewai_tools crewai -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380cc745",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade crewai crewai_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f882f554",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip show crewai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6467fd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install litellm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace50343",
   "metadata": {},
   "source": [
    "## main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8387bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize all your components\n",
    "from crewai import Crew, Process, LLM\n",
    "#import agentops\n",
    "from dotenv import load_dotenv\n",
    "import json \n",
    "#from datetime import datetime\n",
    "# Import your other agents from their respective files\n",
    "from RFP_Strategic_Scout_A1 import selected_agent\n",
    "from Contextual_Analyst_A2 import Extractor_agent\n",
    "from llm_as_judge_A3 import judge_agent\n",
    "import os \n",
    "load_dotenv()\n",
    "\n",
    "#LLM\n",
    "\n",
    "groq_llm = LLM(\n",
    "    model=\"groq/llama-3.1-8b-instant\",\n",
    "    temperature=0.1\n",
    ")\n",
    "groq_llm2 = LLM(\n",
    "    model=\"groq/llama-3.3-70b-versatile\",\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "openai_llm = LLM(\n",
    "    model=\"openai/gpt-4o-mini\", \n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "\n",
    "#main.py\n",
    "def main():\n",
    "\n",
    "    CURRENT_DIR = os.getcwd() \n",
    "    BASE_DIR = os.path.dirname(CURRENT_DIR) \n",
    "    md_path = os.path.join(BASE_DIR, \"data\", \"processed\", \"RFP_Final_Cleaned.md\")\n",
    "    output_dir = os.path.join(BASE_DIR, \"outputs\")\n",
    "    #Agents\n",
    "    selected= selected_agent(llm=openai_llm,md_path=md_path,output_dir=output_dir)\n",
    "    Extractor= Extractor_agent(llm=openai_llm,md_path=md_path,output_dir=output_dir)\n",
    "    judge=judge_agent(llm=openai_llm,output_dir=output_dir)\n",
    "    Extractor.set_context_dependency(selected.get_task)\n",
    "    judge.set_context_dependency(Extractor.get_task)\n",
    "    \n",
    "\n",
    "    # Create the crew\n",
    "    crew = Crew(\n",
    "        agents=[\n",
    "            selected.get_agent,\n",
    "            Extractor.get_agent,\n",
    "            judge.get_agent\n",
    "    \n",
    "        ],\n",
    "        tasks=[\n",
    "            selected.get_task,\n",
    "            Extractor.get_task,\n",
    "            judge.get_task\n",
    "            \n",
    "        ],\n",
    "        process=Process.sequential,\n",
    "        max_rpm=2,\n",
    "        \n",
    "     \n",
    "    )\n",
    "    \n",
    "   \n",
    "    result = crew.kickoff()\n",
    "    print(\"=== RESULTS ===\")\n",
    "    print(result)\n",
    "    return result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc39e7f",
   "metadata": {},
   "source": [
    "test def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c99a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø¯Ø§Ù„Ø© Ù„ØªÙƒÙˆÙ† Ù…Ø³ØªÙ‚Ù„Ø© Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø± (Ø­Ø°Ù self)\n",
    "\n",
    "\n",
    "def cut_full_text(md_path: Path, titles: list):\n",
    "    \"\"\"\n",
    "    Extracts the full text content of specific sections from the RFP markdown file.\n",
    "    \"\"\"\n",
    "    # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ù…Ø³Ø§Ø± Ù…ÙˆØ¬ÙˆØ¯\n",
    "    if not Path(md_path).exists():\n",
    "        return f\"Error: MD file not found at {md_path}\"\n",
    "        \n",
    "    with open(md_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        \n",
    "    extracted = []\n",
    "    for title in titles:\n",
    "        clean_title = title.replace('#', '').strip()\n",
    "\n",
    "        # Ø¨Ø§ØªØ±Ù† Ù…Ø±Ù† Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ†\n",
    "        pattern = rf\"(?ms)^#*\\s*.*{re.escape(clean_title)}.*?\\n(.*?)(?=\\n#|\\n\\d+\\s*-\\s|\\Z)\"\n",
    "        match = re.search(pattern, content)\n",
    "        if match: \n",
    "            text_content = match.group(1).strip()\n",
    "            extracted.append(f\"SECTION: {title}\\nCONTENT:\\n{text_content}\")\n",
    "        else:\n",
    "            extracted.append(f\"SECTION: {title}\\nCONTENT: [Text not found]\")\n",
    "\n",
    "    return \"\\n\\n\" + \"=\"*30 + \"\\n\\n\".join(extracted)\n",
    "\n",
    "\n",
    "\n",
    "# --- 2. Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª ---\n",
    "mdpath = r\"C:\\Users\\user\\OneDrive - University of Prince Mugrin\\Ø³Ø·Ø­ Ø§Ù„Ù…ÙƒØªØ¨\\Capstone_Project_SDAIA\\src\\data\\processed\\RFP_Final_Cleaned.md\"\n",
    "json_path = r\"C:\\Users\\user\\OneDrive - University of Prince Mugrin\\Ø³Ø·Ø­ Ø§Ù„Ù…ÙƒØªØ¨\\Capstone_Project_SDAIA\\src\\outputs\\selected_sections.json\"\n",
    "\n",
    "# --- 3. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† Ù…Ù† Ù…Ù„Ù Ø§Ù„Ù€ JSON Ø£ÙˆÙ„Ø§Ù‹ ---\n",
    "try:\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØµÙÙˆÙØ© Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† (ØªØ£ÙƒØ¯ÙŠ Ù…Ù† Ù…Ø·Ø§Ø¨Ù‚Ø© Ù…ÙØªØ§Ø­ JSONØŒ Ù‡Ù†Ø§ Ø§ÙØªØ±Ø¶Øª Ø£Ù†Ù‡ 'selected_sections')\n",
    "        # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù€ JSON Ø¹Ø¨Ø§Ø±Ø© Ø¹Ù† Ù‚Ø§Ø¦Ù…Ø© Ù…Ø¨Ø§Ø´Ø±Ø© Ø§Ø³ØªØ®Ø¯Ù…ÙŠ: titles_list = [item['section_name'] for item in data]\n",
    "        titles_list = [item['section_name'] for item in data.get('selected_sections', [])]\n",
    "except Exception as e:\n",
    "    print(f\"Error loading JSON: {e}\")\n",
    "    titles_list = []\n",
    "\n",
    "# --- 4. Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ø¯Ø§Ù„Ø© Ø¨Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµØ­ÙŠØ­Ø© ---\n",
    "if titles_list:\n",
    "    result = cut_full_text(mdpath, titles_list)\n",
    "    print(\"=== Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø© ===\")\n",
    "    print(result)\n",
    "else:\n",
    "    print(\"Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† ÙØ§Ø±ØºØ©ØŒ ØªØ£ÙƒØ¯ÙŠ Ù…Ù† Ù…Ù„Ù JSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6423576",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdpath=r\"C:\\Users\\user\\OneDrive - University of Prince Mugrin\\Ø³Ø·Ø­ Ø§Ù„Ù…ÙƒØªØ¨\\Capstone_Project_SDAIA\\src\\data\\processed\\RFP_cleaned.md\"\n",
    "title=r\"C:\\Users\\user\\OneDrive - University of Prince Mugrin\\Ø³Ø·Ø­ Ø§Ù„Ù…ÙƒØªØ¨\\Capstone_Project_SDAIA\\src\\outputs\\Filter_sections.json\"\n",
    "a=cut_full_text(mdpath,title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c10ada4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "def get_contextual_toc(md_path):\n",
    "    with open(md_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† (Markdown # Ø£Ùˆ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ù…Ø«Ù„ 1 -)\n",
    "    # Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…Ø· ÙŠÙ„ØªÙ‚Ø· Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠØ© ÙˆØ§Ù„Ø±Ù‚Ù…ÙŠØ© ÙÙŠ Ø§Ù„ÙƒØ±Ø§Ø³Ø§Øª Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©\n",
    "    sections = re.split(r'\\n(?=#+\\s|\\d+\\s*-\\s)', content)\n",
    "    \n",
    "    smart_toc = []\n",
    "    \n",
    "    for section in sections:\n",
    "        lines = section.strip().split('\\n')\n",
    "        if not lines:\n",
    "            continue\n",
    "            \n",
    "        title = lines[0].strip()\n",
    "        # Ø¯Ù…Ø¬ Ø§Ù„Ø£Ø³Ø·Ø± Ø§Ù„ØªØ§Ù„ÙŠØ© Ù„Ù„Ø¹Ù†ÙˆØ§Ù† Ù„ØªÙƒÙˆÙŠÙ† Ø§Ù„Ù…Ø¹Ø§ÙŠÙ†Ø© (ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø£Ø³Ø·Ø± Ø§Ù„ÙØ§Ø±ØºØ©)\n",
    "        body_preview = \" \".join([l.strip() for l in lines[1:] if l.strip()])[:1000] \n",
    "        \n",
    "        smart_toc.append({\n",
    "            \"title\": title,\n",
    "            \"preview\": body_preview + \"...\" if body_preview else \"Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù†Øµ Ù…ØªØ§Ø­\"\n",
    "        })\n",
    "        \n",
    "    return smart_toc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4913bc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=r\"C:\\Users\\user\\OneDrive - University of Prince Mugrin\\Ø³Ø·Ø­ Ø§Ù„Ù…ÙƒØªØ¨\\Capstone_Project_SDAIA\\src\\Agents\\request1_final_clean3.md\"\n",
    "result=get_contextual_toc(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a2bff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94d7b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ù…Ù† Ù…Ù„Ù .env\n",
    "load_dotenv()\n",
    "\n",
    "# Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ù…ÙØªØ§Ø­\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# ØªÙ…Ø±ÙŠØ± Ø§Ù„Ù…ÙØªØ§Ø­ Ù„Ù„Ø¹Ù…ÙŠÙ„\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70147b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "try:\n",
    "    host = socket.gethostbyname(\"api.groq.com\")\n",
    "    print(f\"Connection successful: {host}\")\n",
    "except Exception as e:\n",
    "    print(f\"Connection failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3718cf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_sections_content(md_file_path, selected_headings):\n",
    "    with open(md_file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    extracted_data = {}\n",
    "    \n",
    "    # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù…Ù„Ù Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† (Headers) Ø§Ù„ØªÙŠ ØªØ¨Ø¯Ø£ Ø¨Ù€ # Ø£Ùˆ ## Ø£Ùˆ ###\n",
    "    # Ø³Ù†Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† Ø§Ù„Ù…Ø®ØªØ§Ø±Ø© ÙˆÙ†Ø³ØªØ®Ø±Ø¬ Ù…Ø§ ØªØ­ØªÙ‡Ø§\n",
    "    for heading in selected_headings:\n",
    "        # Ù†Ù…Ø· Regex Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø¹Ù†ÙˆØ§Ù† ÙˆÙ…Ø§ Ø¨Ø¹Ø¯Ù‡ Ø­ØªÙ‰ Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ø§Ù„ØªØ§Ù„ÙŠ\n",
    "        # Ù†Ø³ØªØ®Ø¯Ù… re.escape Ù„Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø£ÙŠ Ø±Ù…ÙˆØ² Ø®Ø§ØµØ© ÙÙŠ Ø§Ø³Ù… Ø§Ù„Ø¹Ù†ÙˆØ§Ù†\n",
    "        escaped_heading = re.escape(heading)\n",
    "        pattern = rf\"(^#+.*{escaped_heading}.*$\\n)([\\s\\S]*?)(?=\\n#+|$)\"\n",
    "        \n",
    "        match = re.search(pattern, content, re.MULTILINE | re.IGNORECASE)\n",
    "        if match:\n",
    "            extracted_data[heading] = match.group(2).strip()\n",
    "        else:\n",
    "            extracted_data[heading] = \"Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø­ØªÙˆÙ‰ Ù„Ù‡Ø°Ø§ Ø§Ù„Ù‚Ø³Ù….\"\n",
    "\n",
    "    return extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a21760",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_titles = [\" Ø§Ù„Ø§Ø³ØªØ¨Ø¹Ø§Ø¯ Ù…Ù† Ø§Ù„Ù…Ù†Ø§ÙØ³Ø©\",\" Ø§Ù„Ù…ÙˆØ§ÙÙ‚Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø´Ø±ÙˆØ·\"]\n",
    "filtered_context = get_sections_content(r\"C:\\Users\\user\\OneDrive - University of Prince Mugrin\\Ø³Ø·Ø­ Ø§Ù„Ù…ÙƒØªØ¨\\Capstone_Project_SDAIA\\src\\Agents\\request1_final_clean3.md\", selected_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004ac2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a5d88d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a1370a",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ GOOGLE_API_KEY. ØªØ£ÙƒØ¯ Ù…Ù† Ø¥Ø¶Ø§ÙØªÙ‡ Ù„Ù…Ù„Ù .env\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d1204d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"unstructured[md]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664b9cb2",
   "metadata": {},
   "source": [
    "read\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fef1a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from agentic_doc.parse import parse\n",
    "\n",
    "# 1. ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª\n",
    "pdf_path = r\"C:\\Users\\user\\OneDrive - University of Prince Mugrin\\Ø³Ø·Ø­ Ø§Ù„Ù…ÙƒØªØ¨\\Capstone_Project_SDAIA\\src\\data\\request1.pdf\"\n",
    "output_folder = r\"C:\\Users\\user\\OneDrive - University of Prince Mugrin\\Ø³Ø·Ø­ Ø§Ù„Ù…ÙƒØªØ¨\\Capstone_Project_SDAIA\\src\\data\\processed\"\n",
    "\n",
    "# Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# 2. Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ­ÙˆÙŠÙ„\n",
    "print(\"Ø¬Ø§Ø±ÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…Ù„Ù Ù…Ù† PDF Ø¥Ù„Ù‰ MD...\")\n",
    "results = parse([pdf_path],extract_images=False)\n",
    "\n",
    "# 3. Ø­ÙØ¸ Ø§Ù„Ù†ØªÙŠØ¬Ø© ÙƒÙ…Ù„Ù .md\n",
    "# Ù†ÙØªØ±Ø¶ Ø£Ù† results Ø¹Ø¨Ø§Ø±Ø© Ø¹Ù† Ù‚Ø§Ø¦Ù…Ø© ÙƒØ§Ø¦Ù†Ø§ØªØŒ Ù†Ø£Ø®Ø° Ø§Ù„Ù†Øµ Ù…Ù† Ø§Ù„Ø¹Ù†ØµØ± Ø§Ù„Ø£ÙˆÙ„\n",
    "md_filename = \"request1_raw.md\"\n",
    "md_output_path = os.path.join(output_folder, md_filename)\n",
    "\n",
    "with open(md_output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    # Ù…Ù„Ø§Ø­Ø¸Ø©: ØªØ£ÙƒØ¯ Ù…Ù† Ù…Ø®Ø±Ø¬Ø§Øª Ù…ÙƒØªØ¨Ø© agentic_docØŒ Ø¹Ø§Ø¯Ø© Ø§Ù„Ù†Øµ ÙŠÙƒÙˆÙ† ÙÙŠ .text Ø£Ùˆ .markdown\n",
    "    f.write(results[0].markdown) \n",
    "\n",
    "print(f\"âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø®Ø§Ù… ÙÙŠ: {md_output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed17333c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68059763",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06bd7d2a",
   "metadata": {},
   "source": [
    "clean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448ef464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from unstructured.partition.md import partition_md\n",
    "from unstructured.cleaners.core import clean_extra_whitespace\n",
    "\n",
    "def auto_clean_with_unstructured(file_path):\n",
    "    # 1. ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù…Ù„Ù Ø¥Ù„Ù‰ Ø¹Ù†Ø§ØµØ±\n",
    "    elements = partition_md(filename=file_path)\n",
    "\n",
    "    cleaned_text = []\n",
    "\n",
    "    for element in elements:\n",
    "        text = element.text.strip()\n",
    "        \n",
    "        # --- Ø§Ù„ÙÙ„ØªØ±Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¨Ø¯Ø§ÙŠØ© Ø§Ù„Ù†Øµ (Ø£ÙˆØµØ§Ù Ø§Ù„ØµÙˆØ± ÙˆØ§Ù„Ù„ÙˆÙ‚Ùˆ) ---\n",
    "        image_keywords = [\n",
    "            \"Summary :\", \"logo:\", \"Visible Elements :\", \"Placement & Dimensions :\", \n",
    "            \"Analysis :\", \"Graphic Elements :\", \"Text Elements :\", \"Design & Placement :\", \n",
    "            \"Design & Layout :\", \"Layout :\", \"Design Elements :\", \"Design Details :\", \n",
    "            \"Text Fields :\", \"Colour Palette :\", \"Spatial Relationships :\",\"Dimensions & Placement :\",\"/25, 4:29 PM\",\"ØªØ§Ø±ÙŠØ® Ø·Ø±Ø­ Ø§Ù„ÙƒØ±Ø§Ø³Ø©: /1446\",\"/1446\"\n",
    "        ]\n",
    "        \n",
    "        if any(text.startswith(key) for key in image_keywords):\n",
    "            continue \n",
    "\n",
    "        # --- Ø§Ù„ÙÙ„ØªØ±Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ ÙˆØ¬ÙˆØ¯ ÙƒÙ„Ù…Ø§Øª Ù…Ø¹ÙŠÙ†Ø© (Ø§Ù„ØªØ±ÙˆÙŠØ³Ø§Øª ÙˆØ§Ù„ØªØ°ÙŠÙŠÙ„Ø§Øª) ---\n",
    "        excluded_headers = [\n",
    "            \"Ø§Ù„Ù…Ù…Ù„ÙƒØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©\", \"ÙƒØ±Ø§Ø³Ø© Ø§Ù„Ø´Ø±ÙˆØ·\", \"ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¥ØµØ¯Ø§Ø±\", \n",
    "            \"tenders.etimad.sa\", \"Ø§Ù„Ù…Ø¹ØªÙ…Ø¯ Ø¨Ù…ÙˆØ¬Ø¨ Ù‚Ø±Ø§Ø± ÙˆØ²ÙŠØ± Ø§Ù„Ù…Ø§Ù„ÙŠØ©\",\"Ø§Ø³Ù… Ø§Ù„Ù…Ù†Ø§ÙØ³Ø©: \",\"Ø±Ù‚Ù… Ø§Ù„ÙƒØ±Ø§Ø³Ø©: \",\"ØªØ§Ø±ÙŠØ® Ø·Ø±Ø­ Ø§Ù„ÙƒØ±Ø§Ø³Ø©:\" ,\"/25, 4:29 PM\"\n",
    "        ]\n",
    "        if any(key in text for key in excluded_headers):\n",
    "            continue\n",
    "\n",
    "        # --- ØªØµØ­ÙŠØ­ Ø­Ø°Ù Ø£Ø±Ù‚Ø§Ù… Ø§Ù„ØµÙØ­Ø§Øª ÙˆØ§Ù„Ø±ÙˆØ§Ø¨Ø· Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Regex ---\n",
    "        # Ø­Ø°Ù Ø£Ø±Ù‚Ø§Ù… Ø§Ù„ØµÙØ­Ø§Øª Ù…Ø«Ù„ 31/37\n",
    "        text = re.sub(r'\\d+/\\d+', '', text)\n",
    "        \n",
    "        # Ø­Ø°Ù Ø£ÙŠ Ø±Ø§Ø¨Ø· ÙŠØ¨Ø¯Ø£ Ø¨Ù€ http (Ø±ÙˆØ§Ø¨Ø· Ù…Ù†ØµØ© Ø§Ø¹ØªÙ…Ø§Ø¯)\n",
    "        text = re.sub(r'https?://\\S+', '', text)\n",
    "\n",
    "        # 4. ØªÙ†Ø¸ÙŠÙ Ø§Ù„ÙØ±Ø§ØºØ§Øª Ø§Ù„Ù†Ø§ØªØ¬Ø© Ø¹Ù† Ø§Ù„Ø­Ø°Ù\n",
    "        text = clean_extra_whitespace(text)\n",
    "\n",
    "        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù†Øµ Ø§Ù„Ù†Ø¸ÙŠÙ ÙÙ‚Ø· Ø¥Ø°Ø§ Ù„Ù… ÙŠØµØ¨Ø­ ÙØ§Ø±ØºØ§Ù‹ Ø¨Ø¹Ø¯ Ø§Ù„Ø­Ø°Ù\n",
    "        if text.strip():\n",
    "            cleaned_text.append(text)\n",
    "\n",
    "    return \"\\n\\n\".join(cleaned_text)\n",
    "\n",
    "# --- Ø§Ù„ØªØ´ØºÙŠÙ„ ---\n",
    "input_file = r\"C:\\Users\\user\\OneDrive - University of Prince Mugrin\\Ø³Ø·Ø­ Ø§Ù„Ù…ÙƒØªØ¨\\Capstone_Project_SDAIA\\src\\data\\processed\\request1_raw.md\"\n",
    "final_content = auto_clean_with_unstructured(input_file)\n",
    "\n",
    "with open(\"request1_final_clean3.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(final_content)\n",
    "\n",
    "print(\"âœ… ØªÙ… ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ù„Ù Ø¨Ù†Ø¬Ø§Ø­ ÙˆØ­Ø°Ù Ø£ÙˆØµØ§Ù Ø§Ù„ØµÙˆØ± ÙˆØ§Ù„Ø±ÙˆØ§Ø¨Ø· ÙˆØ£Ø±Ù‚Ø§Ù… Ø§Ù„ØµÙØ­Ø§Øª!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6b049c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_toc_only(file_path, limit_lines=145):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = [next(f) for _ in range(limit_lines)]\n",
    "    return \"\".join(lines)\n",
    "\n",
    "# Ø«Ù… Ù†Ù…Ø±Ø± Ù‡Ø°Ø§ Ø§Ù„Ù†Øµ Ù„Ù„Ù€ Task Ù…Ø¨Ø§Ø´Ø±Ø©\n",
    "toc_content = read_toc_only(r\"C:\\Users\\user\\OneDrive - University of Prince Mugrin\\Ø³Ø·Ø­ Ø§Ù„Ù…ÙƒØªØ¨\\Capstone_Project_SDAIA\\src\\Agents\\request1_final_clean3.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ac762e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(toc_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb5fee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def read_toc_to_json(file_path, output_json_path, limit_lines=145):\n",
    "    lines = []\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            # Ù†Ù‚Ø±Ø£ Ø§Ù„Ø£Ø³Ø·Ø± ÙˆÙ†Ù‚ÙˆÙ… Ø¨ØªÙ†Ø¸ÙŠÙÙ‡Ø§ Ù…Ù† Ø§Ù„ÙØ±Ø§ØºØ§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©\n",
    "            for _ in range(limit_lines):\n",
    "                line = next(f).strip()\n",
    "                if line: # Ù†ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø£Ø³Ø·Ø± Ø§Ù„ÙØ§Ø±ØºØ© ØªÙ…Ø§Ù…Ø§Ù‹ Ù„ØªÙ‚Ù„ÙŠÙ„ Ø­Ø¬Ù… Ø§Ù„Ù…Ù„Ù\n",
    "                    lines.append(line)\n",
    "    except StopIteration:\n",
    "        pass # Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„Ù…Ù„Ù Ù‚Ø¨Ù„ Ø§Ù„ÙˆØµÙˆÙ„ Ù„Ù„Ø­Ø¯ Ø§Ù„Ù…Ø­Ø¯Ø¯\n",
    "    except Exception as e:\n",
    "        print(f\"Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ù‚Ø±Ø§Ø¡Ø©: {e}\")\n",
    "        return None\n",
    "\n",
    "    # ØªØ´ÙƒÙŠÙ„ Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ù€ JSON\n",
    "    toc_data = {\n",
    "        \"document_name\": os.path.basename(file_path),\n",
    "        \"toc_lines\": lines  # Ù‡Ù†Ø§ Ù†Ø¶Ø¹ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø©\n",
    "    }\n",
    "\n",
    "    # Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù Ø¨ØµÙŠØºØ© JSON\n",
    "    with open(output_json_path, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(toc_data, json_file, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    print(f\"âœ… ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙÙ‡Ø±Ø³ ÙˆØ­ÙØ¸Ù‡ ÙÙŠ: {output_json_path}\")\n",
    "    return toc_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b899c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from unstructured.partition.md import partition_md\n",
    "from unstructured.cleaners.core import clean_extra_whitespace\n",
    "\n",
    "def auto_clean_with_unstructured(file_path):\n",
    "    elements = partition_md(filename=file_path)\n",
    "    cleaned_text = []\n",
    "\n",
    "    for element in elements:\n",
    "        text = element.text.strip()\n",
    "        \n",
    "        # --- Ø§Ù„ÙÙ„ØªØ±Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø³ØªØ¨Ø¹Ø¯Ø© ---\n",
    "        image_keywords = [\n",
    "            \"Summary :\", \"logo:\", \"Visible Elements :\", \"Placement & Dimensions :\", \n",
    "            \"Analysis :\", \"Graphic Elements :\", \"Text Elements :\", \"Design & Placement :\", \n",
    "            \"Design & Layout :\", \"Layout :\", \"Design Elements :\", \"Design Details :\", \n",
    "            \"Text Fields :\", \"Colour Palette :\", \"Spatial Relationships :\",\"Dimensions & Placement :\",\"/25, 4:29 PM\",\"ØªØ§Ø±ÙŠØ® Ø·Ø±Ø­ Ø§Ù„ÙƒØ±Ø§Ø³Ø©: /1446\",\"/1446\"\n",
    "        ]        \n",
    "        if any(text.startswith(key) for key in image_keywords): continue \n",
    "\n",
    "        excluded_headers = [\n",
    "            \"Ø§Ù„Ù…Ù…Ù„ÙƒØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©\", \"ÙƒØ±Ø§Ø³Ø© Ø§Ù„Ø´Ø±ÙˆØ·\", \"ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¥ØµØ¯Ø§Ø±\", \n",
    "            \"tenders.etimad.sa\", \"Ø§Ù„Ù…Ø¹ØªÙ…Ø¯ Ø¨Ù…ÙˆØ¬Ø¨ Ù‚Ø±Ø§Ø± ÙˆØ²ÙŠØ± Ø§Ù„Ù…Ø§Ù„ÙŠØ©\",\"Ø§Ø³Ù… Ø§Ù„Ù…Ù†Ø§ÙØ³Ø©: \",\"Ø±Ù‚Ù… Ø§Ù„ÙƒØ±Ø§Ø³Ø©: \",\"ØªØ§Ø±ÙŠØ® Ø·Ø±Ø­ Ø§Ù„ÙƒØ±Ø§Ø³Ø©:\" ,\"/25, 4:29 PM\"\n",
    "        ]        \n",
    "        if any(key in text for key in excluded_headers): continue\n",
    "\n",
    "        # --- Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ù†Ø¸Ù ---\n",
    "        text = re.sub(r'\\d+/\\d+', '', text)\n",
    "        text = re.sub(r'https?://\\S+', '', text)\n",
    "        text = clean_extra_whitespace(text)\n",
    "\n",
    "        if text.strip():\n",
    "            # --- Ù…ÙŠØ²Ø© Ø§Ø³ØªØ¹Ø§Ø¯Ø© Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† (The Fix) ---\n",
    "            # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø·Ø± ÙŠØ¨Ø¯Ø£ Ø¨Ù†Ù…Ø· Ø¹Ù†ÙˆØ§Ù† (Ø±Ù‚Ù… - Ù†Øµ) ÙˆÙ„Ù… ÙŠÙƒÙ† Ù…ØµÙ†ÙØ§Ù‹ ÙƒØ¹Ù†ÙˆØ§Ù†ØŒ Ù†Ø­ÙˆÙ„Ù‡ ÙŠØ¯ÙˆÙŠØ§Ù‹\n",
    "            # Ø§Ù„Ù†Ù…Ø·: Ø±Ù‚Ù… Ù…ØªØ¨ÙˆØ¹ Ø¨Ù…Ø³Ø§ÙØ© Ø«Ù… Ø´Ø±Ø·Ø© Ø«Ù… Ù…Ø³Ø§ÙØ© (Ù…Ø«Ù„Ø§Ù‹: 5 - Ø£Ù‡Ù„ÙŠØ©)\n",
    "            heading_pattern = r'^(\\d+\\s*-\\s+.*|^Ø§Ù„Ù‚Ø³Ù…\\s+.*|^Ø§Ù„Ù…Ø§Ø¯Ø©\\s+.*)'\n",
    "            \n",
    "            if re.match(heading_pattern, text):\n",
    "                # Ù†ØªØ£ÙƒØ¯ Ø£Ù†Ù‡ Ù„ÙŠØ³ Ø¶Ù…Ù† Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† Ø§Ù„Ù…Ø­ÙˆÙ„Ø© Ù…Ø³Ø¨Ù‚Ø§Ù‹ Ù„Ù…Ù†Ø¹ Ø§Ù„ØªÙƒØ±Ø§Ø±\n",
    "                formatted_text = f\"## {text}\" \n",
    "                cleaned_text.append(formatted_text)\n",
    "            else:\n",
    "                cleaned_text.append(text)\n",
    "\n",
    "    return \"\\n\\n\".join(cleaned_text)\n",
    "\n",
    "\n",
    "# --- Ø§Ù„ØªØ´ØºÙŠÙ„ ---\n",
    "input_file = r\"C:\\Users\\user\\OneDrive - University of Prince Mugrin\\Ø³Ø·Ø­ Ø§Ù„Ù…ÙƒØªØ¨\\Capstone_Project_SDAIA\\src\\data\\processed\\request1_raw.md\"\n",
    "final_content = auto_clean_with_unstructured(input_file)\n",
    "\n",
    "with open(\"request1_final_clean3.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(final_content)\n",
    "\n",
    "print(\"âœ… ØªÙ… ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ù„Ù Ø¨Ù†Ø¬Ø§Ø­ ÙˆØ­Ø°Ù Ø£ÙˆØµØ§Ù Ø§Ù„ØµÙˆØ± ÙˆØ§Ù„Ø±ÙˆØ§Ø¨Ø· ÙˆØ£Ø±Ù‚Ø§Ù… Ø§Ù„ØµÙØ­Ø§Øª!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c636cd71",
   "metadata": {},
   "source": [
    "## propsal_convertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7766b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2883acb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from agentic_doc.parse import parse\n",
    "from unstructured.partition.md import partition_md\n",
    "from unstructured.cleaners.core import clean_extra_whitespace\n",
    "\n",
    "class DocumentProcessor:\n",
    "    def __init__(self, output_folder):\n",
    "        self.output_folder = output_folder\n",
    "        if not os.path.exists(self.output_folder):\n",
    "            os.makedirs(self.output_folder)\n",
    "\n",
    "    def process_pdf_to_clean_md(self, pdf_path):\n",
    "        \"\"\"\n",
    "        Ø§Ù„Ù…Ù‡Ù…Ø©: ØªØ­ÙˆÙŠÙ„ PDF Ø¥Ù„Ù‰ MD Ø«Ù… ØªÙ†Ø¸ÙŠÙÙ‡ ÙˆØ¥ØµÙ„Ø§Ø­ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† ÙÙŠ Ø®Ø·ÙˆØ© ÙˆØ§Ø­Ø¯Ø©.\n",
    "        \"\"\"\n",
    "        file_base_name = os.path.basename(pdf_path).replace(\".pdf\", \"\")\n",
    "        raw_md_path = os.path.join(self.output_folder, f\"{file_base_name}_raw.md\")\n",
    "        final_md_path = os.path.join(self.output_folder, f\"{file_base_name}_final_clean.md\")\n",
    "\n",
    "        # 1. Ø§Ù„Ù…Ø±Ø­Ù„Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰: Ø§Ù„Ù‚Ø±Ø§Ø¡Ø© ÙˆØ§Ù„ØªØ­ÙˆÙŠÙ„ (Parsing)\n",
    "        print(f\"ğŸ”„ Ø¬Ø§Ø±ÙŠ ØªØ­ÙˆÙŠÙ„ {file_base_name} Ù…Ù† PDF Ø¥Ù„Ù‰ Markdown...\")\n",
    "        results = parse([pdf_path])\n",
    "        raw_content = results[0].markdown\n",
    "\n",
    "        with open(raw_md_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(raw_content)\n",
    "\n",
    "        # 2. Ø§Ù„Ù…Ø±Ø­Ù„Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ©: Ø§Ù„ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù‡ÙŠÙƒÙ„ÙŠ (Cleaning & Repairing)\n",
    "        print(f\"ğŸ§¹ Ø¬Ø§Ø±ÙŠ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ ÙˆØ¥ØµÙ„Ø§Ø­ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† (Ø¨Ù…Ø§ ÙÙŠ Ø°Ù„Ùƒ Ø¹Ù†ÙˆØ§Ù† 64)...\")\n",
    "        cleaned_content = self._auto_clean_logic(raw_md_path)\n",
    "\n",
    "        with open(final_md_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(cleaned_content)\n",
    "\n",
    "        print(f\"âœ… Ø§ÙƒØªÙ…Ù„Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©! Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù†Ø¸ÙŠÙ: {final_md_path}\")\n",
    "        return final_md_path\n",
    "\n",
    "    def _auto_clean_logic(self, file_path):\n",
    "        \"\"\"Ù…Ù†Ø·Ù‚ Ø§Ù„ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¹Ù…ÙŠÙ‚ ÙˆØ§Ø³ØªØ¹Ø§Ø¯Ø© Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ†\"\"\"\n",
    "        elements = partition_md(filename=file_path)\n",
    "        cleaned_text = []\n",
    "\n",
    "        # ÙƒÙ„Ù…Ø§Øª Ù…Ø³ØªØ¨Ø¹Ø¯Ø© (Ø§Ù„ØµÙˆØ± ÙˆØ§Ù„ØªØ±ÙˆÙŠØ³Ø§Øª)\n",
    "        image_keywords = [\n",
    "            \"Summary :\", \"logo:\", \"Visual Elements  :\", \"Placement & Dimensions :\", \n",
    "            \"Analysis :\", \"Graphic Elements :\", \"Text Elements :\", \"Design & Placement :\", \n",
    "            \"Design & Layout :\", \"Layout :\", \"Design Elements :\", \"Design Details :\", \n",
    "            \"Text Fields :\", \"Colour Palette :\", \"Spatial Relationships :\",\"Dimensions & Placement :\",\"/25, 4:29 PM\",\"ØªØ§Ø±ÙŠØ® Ø·Ø±Ø­ Ø§Ù„ÙƒØ±Ø§Ø³Ø©: /1446\",\"/1446\",\"Technical Details :\",\"Design & Colour :\",\"Logo Design :\",\"Text Details :\"\n",
    "        ]   \n",
    "        excluded_headers = [\n",
    "            \"Ø§Ù„Ù…Ù…Ù„ÙƒØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©\", \"ÙƒØ±Ø§Ø³Ø© Ø§Ù„Ø´Ø±ÙˆØ·\", \"ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¥ØµØ¯Ø§Ø±\", \n",
    "            \"tenders.etimad.sa\", \"Ø§Ù„Ù…Ø¹ØªÙ…Ø¯ Ø¨Ù…ÙˆØ¬Ø¨ Ù‚Ø±Ø§Ø± ÙˆØ²ÙŠØ± Ø§Ù„Ù…Ø§Ù„ÙŠØ©\",\"Ø§Ø³Ù… Ø§Ù„Ù…Ù†Ø§ÙØ³Ø©: \",\"Ø±Ù‚Ù… Ø§Ù„ÙƒØ±Ø§Ø³Ø©: \",\"ØªØ§Ø±ÙŠØ® Ø·Ø±Ø­ Ø§Ù„ÙƒØ±Ø§Ø³Ø©:\" ,\"/25, 4:29 PM\"\n",
    "        ]    \n",
    "\n",
    "        for element in elements:\n",
    "            text = element.text.strip()\n",
    "            \n",
    "            # Ø§Ù„ÙÙ„ØªØ±Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø³ØªØ¨Ø¹Ø¯Ø©\n",
    "            if any(text.startswith(key) for key in image_keywords): continue \n",
    "            if any(key in text for key in excluded_headers): continue\n",
    "\n",
    "            # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· ÙˆØ£Ø±Ù‚Ø§Ù… Ø§Ù„ØµÙØ­Ø§Øª\n",
    "            text = re.sub(r'\\d+/\\d+', '', text)\n",
    "            text = re.sub(r'https?://\\S+', '', text)\n",
    "            text = clean_extra_whitespace(text)\n",
    "\n",
    "            if text.strip():\n",
    "                # --- ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† (Fixing Heading 64 and others) ---\n",
    "                # Ø§Ù„Ù†Ù…Ø· Ø§Ù„Ø¬Ø¯ÙŠØ¯: ÙŠØ¨Ø­Ø« Ø¹Ù† Ø±Ù‚Ù… ÙÙŠ Ø¨Ø¯Ø§ÙŠØ© Ø§Ù„Ø³Ø·Ø± ÙŠØªØ¨Ø¹Ù‡ (Ø´Ø±Ø·Ø© Ø£Ùˆ Ù†Ù‚Ø·Ø© Ø£Ùˆ Ù…Ø³Ø§ÙØ©)\n",
    "                # Ù…Ø«Ø§Ù„: \"64 Ø¹Ù†ÙˆØ§Ù†\" Ø£Ùˆ \"64 - Ø¹Ù†ÙˆØ§Ù†\" Ø£Ùˆ \"64. Ø¹Ù†ÙˆØ§Ù†\"\n",
    "                heading_pattern = r'^(\\d+[\\s\\.\\-].*|^Ø§Ù„Ù‚Ø³Ù…\\s+.*|^Ø§Ù„Ù…Ø§Ø¯Ø©\\s+.*)'\n",
    "                \n",
    "                if re.match(heading_pattern, text):\n",
    "                    # Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ø§Ù„Ø³Ø·Ø± ÙŠØ­Ù…Ù„ Ø¹Ù„Ø§Ù…Ø© Ø§Ù„Ø¹Ù†ÙˆØ§Ù† #ØŒ Ù†Ø¶ÙŠÙÙ‡Ø§ Ù„Ù‡\n",
    "                    if not text.startswith('#'):\n",
    "                        text = f\"## {text}\"\n",
    "                \n",
    "                cleaned_text.append(text)\n",
    "\n",
    "        return \"\\n\\n\".join(cleaned_text)\n",
    "\n",
    "# --- Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙÙŠ Ù…Ù„ÙÙƒ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ ---\n",
    "if __name__ == \"__main__\":\n",
    "    PDF_INPUT = r\"C:\\Users\\user\\OneDrive - University of Prince Mugrin\\Ø³Ø·Ø­ Ø§Ù„Ù…ÙƒØªØ¨\\Capstone_Project_SDAIA\\src\\data\\RFP.pdf\"\n",
    "    OUTPUT_DIR = r\"C:\\Users\\user\\OneDrive - University of Prince Mugrin\\Ø³Ø·Ø­ Ø§Ù„Ù…ÙƒØªØ¨\\Capstone_Project_SDAIA\\src\\data\\processed\"\n",
    "    \n",
    "    processor = DocumentProcessor(OUTPUT_DIR)\n",
    "    final_file = processor.process_pdf_to_clean_md(PDF_INPUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1927aebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from proposal_processor import run_pipeline  # Ensure your file is named proposal_processor.py\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "RAW_DIR = r\"D:\\Capstone_Project_SDAIA\\src\\data\\raw\"\n",
    "PROCESSED_DIR = r\"D:\\Capstone_Project_SDAIA\\src\\data\\processed\"\n",
    "\n",
    "def process_all_vendors():\n",
    "    # 1. Create output directory if it doesn't exist\n",
    "    if not os.path.exists(PROCESSED_DIR):\n",
    "        os.makedirs(PROCESSED_DIR)\n",
    "        print(f\"ğŸ“ Created directory: {PROCESSED_DIR}\")\n",
    "\n",
    "    # 2. Find all files starting with \"Vendor\" (case-insensitive search)\n",
    "    # This captures .pdf and .docx files\n",
    "    search_pattern = os.path.join(RAW_DIR, \"Vendor*.*\")\n",
    "    vendor_files = glob.glob(search_pattern)\n",
    "\n",
    "    if not vendor_files:\n",
    "        print(f\"âš ï¸ No files starting with 'Vendor' found in {RAW_DIR}\")\n",
    "        return\n",
    "\n",
    "    print(f\"ğŸ” Found {len(vendor_files)} vendor files to process.\")\n",
    "\n",
    "    # 3. Loop through each file and run your chunked cleaning pipeline\n",
    "    for file_path in vendor_files:\n",
    "        try:\n",
    "            print(f\"\\n--- Processing: {os.path.basename(file_path)} ---\")\n",
    "            \n",
    "            # This triggers your gpt-4o-mini chunked refinement\n",
    "            final_path = run_pipeline(file_path, PROCESSED_DIR)\n",
    "            \n",
    "            print(f\"âœ… Success! Cleaned Markdown saved at: {final_path}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Catches errors like the 30k token limit or API issues\n",
    "            print(f\"âŒ Failed to process {os.path.basename(file_path)}: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_all_vendors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd27e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from crewai import Crew, Process\n",
    "from langchain_openai import ChatOpenAI\n",
    "from Response_analyst_A3 import ResponseAnalyst # Ensure this matches your file name\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# The input is the output of your proposal_processor\n",
    "INPUT_DIR = r\"D:\\Capstone_Project_SDAIA\\src\\data\\processed\"\n",
    "# The requirements come from Agent 3 (LLM as Judge)\n",
    "REQUIREMENTS_JSON = r\"D:\\Capstone_Project_SDAIA\\src\\outputs\\strategic_refined_requirements.json\"\n",
    "# Final evidence location\n",
    "OUTPUT_DIR = r\"D:\\Capstone_Project_SDAIA\\src\\outputs\"\n",
    "\n",
    "def run_batch_analysis():\n",
    "    # 1. Initialize the LLM (Using gpt-4o-mini to stay under 30k token limit)\n",
    "    llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "    # 2. Find all files starting with \"Vendor\" and ending with \"cleaned.md\"\n",
    "    search_pattern = os.path.join(INPUT_DIR, \"Vendor*cleaned.md\")\n",
    "    cleaned_files = glob.glob(search_pattern)\n",
    "\n",
    "    if not cleaned_files:\n",
    "        print(f\"âš ï¸ No cleaned vendor files found in {INPUT_DIR}\")\n",
    "        return\n",
    "\n",
    "    print(f\"ğŸ” Found {len(cleaned_files)} cleaned proposals to analyze.\")\n",
    "\n",
    "    # 3. Iterate through each vendor file\n",
    "    for md_file in cleaned_files:\n",
    "        vendor_filename = os.path.basename(md_file)\n",
    "        print(f\"\\nğŸš€ Starting RAG Extraction for: {vendor_filename}\")\n",
    "\n",
    "        try:\n",
    "            # Initialize your Agent 4 class\n",
    "            # It uses the Judge's output as the source of truth for requirements\n",
    "            analyst_system = ResponseAnalyst(\n",
    "                llm=llm,\n",
    "                proposal_md_path=md_file,\n",
    "                requirements_json_path=REQUIREMENTS_JSON,\n",
    "                output_dir=OUTPUT_DIR\n",
    "            )\n",
    "\n",
    "            # Wrap in a Crew for execution\n",
    "            crew = Crew(\n",
    "                agents=[analyst_system.agent],\n",
    "                tasks=[analyst_system.task],\n",
    "                process=Process.sequential,\n",
    "                verbose=True\n",
    "            )\n",
    "\n",
    "            # Kickoff the RAG search\n",
    "            result = crew.kickoff()\n",
    "            \n",
    "            # Validation\n",
    "            evidence_path = os.path.join(OUTPUT_DIR, f\"evidence_{vendor_filename}.json\")\n",
    "            if os.path.exists(evidence_path):\n",
    "                print(f\"âœ… Success! Evidence extracted to: {evidence_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error analyzing {vendor_filename}: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_batch_analysis()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
